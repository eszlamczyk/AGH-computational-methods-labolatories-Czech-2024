{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOWNIT LABORATORIUM 6 - SVD zastosowania\n",
    "\n",
    "Opis nie będzie bardzo obfity ponieważ mam nadzieje że więcej będzie widoczne na prezentacji\n",
    "\n",
    "krok 1. Crawlowanie sieci + tworzenie strony: plik lab6_websiteHandler.py\n",
    "\n",
    "Kod ten crawluje linki (getURLList), scrapuje potem te linki (urlFromWebiste) i tworzy odpowiednie słowniki słów (TextToBow, urlToBowList). Ztokenizowanie słów do wielkiego słownika klucz:wartość -> linkDoStrony:słownikSłów (createBOWs). Podczas tego tworze także Inverse document frequency i unie wszystkich słów (CreateBOWs i calculateIDFs). Każda operacja jest na słownikach i zapisywana do odpowiedniego pliku JSON (save_dict_to_file, load_dict_from_file) (ponieważ operacje przypisu i odczytu na hashsetach sa szybsze (O(1)) niż na tablicach (O(N)))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import PriorityQueue\n",
    "import re\n",
    "import multiprocessing\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "def getStopWords(path):\n",
    "    List = []\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            List.append(line.strip())\n",
    "    return List\n",
    "\n",
    "\n",
    "# globals\n",
    "stopWords = getStopWords(\"Databases/stopwords.txt\")\n",
    "\n",
    "\n",
    "def stopWordsRemoval(x):\n",
    "    global stopWords\n",
    "    return not x in stopWords\n",
    "\n",
    "\n",
    "def TextToBOW(text: str) -> dict:\n",
    "    def remove_unicode(text):\n",
    "        return re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "\n",
    "    clean_text = remove_unicode(text.lower())\n",
    "    words = re.split(\n",
    "        r'[\\/;\\'\\n:().,\\s\\[\\]\\-\"`?@! ]+', clean_text)\n",
    "    FilteredText = list(filter(stopWordsRemoval, words))\n",
    "    BOW = {}\n",
    "    for word in FilteredText:\n",
    "        BOW[word] = 1 + BOW.get(word, 0)\n",
    "\n",
    "    return BOW\n",
    "\n",
    "\n",
    "def UrlToWorldList(url):\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "    except:\n",
    "        return\n",
    "    html_bytes = page.read()\n",
    "    try:\n",
    "        html = html_bytes.decode(\"utf-8\")\n",
    "    except:\n",
    "        return\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    text = soup.getText()\n",
    "\n",
    "    return TextToBOW(text)\n",
    "\n",
    "\n",
    "def save_dict_to_file(dictionary, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(dictionary, file)\n",
    "\n",
    "\n",
    "def load_dict_from_file(filename) -> dict:\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def filter_none_values(dictionary: dict):\n",
    "    return {k: v for k, v in dictionary.items() if v is not None}\n",
    "\n",
    "\n",
    "def urlFromWebsite(InitialURL: str, amount: int) -> dict:\n",
    "    print(\"Finding in: \", InitialURL)\n",
    "    URLs = {}\n",
    "    Q = PriorityQueue()\n",
    "    Q.put((1, InitialURL))\n",
    "    URLs[InitialURL] = True\n",
    "    while len(URLs) <= amount and (not Q.empty()):\n",
    "\n",
    "        print(len(URLs))\n",
    "        deph, url = Q.get()\n",
    "        if deph > 3:\n",
    "            continue\n",
    "        try:\n",
    "            page = urlopen(url)\n",
    "        except:\n",
    "            # print(url)\n",
    "            continue\n",
    "\n",
    "        html_bytes = page.read()\n",
    "        try:\n",
    "            html = html_bytes.decode(\"utf-8\")\n",
    "        except:\n",
    "            continue\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        for link in soup.findAll(\"a\"):\n",
    "            parsedLink = link.get('href')\n",
    "            if parsedLink is None:\n",
    "                continue\n",
    "\n",
    "            # links sometimes start with for example \"/news\" or \"//bbc.com/news\" instead of \"https://bbc.com/news\"\n",
    "            # thats why i want to look into both cases, this way it is the fastest\n",
    "\n",
    "            if InitialURL in parsedLink:\n",
    "                pass\n",
    "            elif parsedLink.startswith(\"//\") and parsedLink != \"//\":\n",
    "                parsedLink = \"https:\" + parsedLink\n",
    "            elif parsedLink.startswith(\"/\") and parsedLink != \"/\":\n",
    "                parsedLink = InitialURL + parsedLink\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if not URLs.get(parsedLink, False):\n",
    "                Q.put((deph+1, parsedLink))\n",
    "                URLs[parsedLink] = True\n",
    "\n",
    "    print(\"Parsing: \", InitialURL)\n",
    "    print(\"Warning it might take a lot of time (20mins +)\")\n",
    "\n",
    "    for url in list(URLs.keys()):\n",
    "        URLs[url] = UrlToWorldList(url)\n",
    "\n",
    "    URLs = filter_none_values(URLs)\n",
    "\n",
    "    print(\"Finished looking for URLs in: \",\n",
    "          InitialURL, \". Found \", len(URLs), \" URLs\")\n",
    "    return URLs\n",
    "\n",
    "\n",
    "def process_url(URL, amount, result_dict):\n",
    "    result_dict.update(urlFromWebsite(URL, amount))\n",
    "\n",
    "\n",
    "def getUrlList():\n",
    "    InitialURLs = [(\"https://www.bbc.com\", 1000),\n",
    "                   (\"https://www.nbcnews.com\", 1000),\n",
    "                   (\"https://edition.cnn.com\", 1000),\n",
    "                   (\"https://www.foxnews.com\", 2000),\n",
    "                   (\"https://people.com\", 1500),\n",
    "                   (\"https://www.abc.net.au\", 500),\n",
    "                   (\"https://www.news18.com\", 2000),\n",
    "                   (\"https://eu.usatoday.com\", 700),\n",
    "                   (\"https://news.sky.com\", 1000)]\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    URLs = manager.dict()\n",
    "    jobs = []\n",
    "\n",
    "    for URL, amount in InitialURLs:\n",
    "        p = multiprocessing.Process(\n",
    "            target=process_url, args=(URL, amount, URLs))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for p in jobs:\n",
    "        p.join()\n",
    "\n",
    "    return dict(URLs)\n",
    "\n",
    "\n",
    "def createBOWs():\n",
    "    global globalBow, wordIndex\n",
    "    print(\"================GettingBOWs==============\")\n",
    "\n",
    "    # Get URLs using multiprocessing\n",
    "    URLs = getUrlList()\n",
    "    URLIndexes = {}\n",
    "    URLIndex = 0\n",
    "    globalBOW = {}\n",
    "    for url in list(URLs.keys()):\n",
    "        URLIndexes[url] = URLIndex\n",
    "        URLIndex += 1\n",
    "        globalBOW.update(URLs[url])\n",
    "\n",
    "    BOWIndex = 0\n",
    "    for word in list(globalBOW.keys()):\n",
    "        globalBOW[word] = BOWIndex\n",
    "        BOWIndex += 1\n",
    "\n",
    "    print(globalBOW)\n",
    "    print(BOWIndex)\n",
    "    save_dict_to_file(URLs, \"Databases/URLs.json\")\n",
    "    save_dict_to_file(globalBOW, \"Databases/globalBow.json\")\n",
    "    print(\"=======================DONE=======================\")\n",
    "\n",
    "\n",
    "def calculateIDFs() -> dict:\n",
    "    print(\"=====Calculationg Inverse document frequencies====\")\n",
    "    Occurances = {}\n",
    "    URLs = load_dict_from_file(\"Databases/URLs.json\")\n",
    "    for _, urlDict in URLs.items():\n",
    "        for word in list(urlDict.keys()):\n",
    "            Occurances[word] = 1 + Occurances.get(word, 0)\n",
    "\n",
    "    N = len(URLs)\n",
    "    IDF = {}\n",
    "\n",
    "    for key in Occurances:\n",
    "        IDF[key] = math.log10(N/Occurances[key])\n",
    "\n",
    "    save_dict_to_file(IDF, \"Databases/IDFs.json\")\n",
    "    print(\"=======================DONE=======================\")\n",
    "\n",
    "    return IDF\n",
    "\n",
    "\n",
    "def IndexUrls():\n",
    "    print(\"=========INDEXING URLS=========\")\n",
    "    URLs = load_dict_from_file(\"Databases/URLs.json\")\n",
    "    URLIndexes = {}\n",
    "    IndexedURLs = {}\n",
    "    currIndex = 0\n",
    "    for URL, _ in URLs.items():\n",
    "        URLIndexes[URL] = currIndex\n",
    "        IndexedURLs[currIndex] = URL\n",
    "        currIndex += 1\n",
    "\n",
    "    save_dict_to_file(IndexedURLs, \"Databases/IndexedURLs.json\")\n",
    "    save_dict_to_file(URLIndexes, \"Databases/URLIndexes.json\")\n",
    "    print(\"=======================DONE=======================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krok 2. Operacje na wyszukiwaniach\n",
    "\n",
    "plik - lab6.py\n",
    "\n",
    "Ta część kodu odpowiada za wszystkie operacje na macierzach - używam główne biblioteki scipy.sparse csr_matrix, csc_matrix w zależności od potrzeby (i lil_matrix przy tworzeniu). Dodatkowo także liczę $U_k\\Sigma_k$ i $V_k^T$ używając biblioteki sklearn.decomposition, ponieważ jak podaje dokumentacja: \n",
    "\n",
    "*This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently.*\n",
    "\n",
    "Następnie mamy funkcje\n",
    "```py\n",
    "def PrzeglądarkaMain(TermByDocumentMatrix, InputString: str, URLsLen, SVD=False, OutputSize=10) -> list:\n",
    "```\n",
    "która jest sercem mojej \"wyszukiwarki\". Przyjmuje ona na wejściu string, następnie zamienia ten string na BOW i go normalizuje, poczym wykonuje 2 dostepne kalkulacje wektora podobieństwa:\n",
    "\n",
    "bez SVD:\n",
    "$$|q^TA| = [|cos\\theta_1|,|cos\\theta_2|,...,|cos\\theta_n|]$$\n",
    "\n",
    "oraz z SVD:\n",
    "$$|q^TU_k\\Sigma_kV^T_k| = [|cos\\theta_1|,|cos\\theta_2|,...,|cos\\theta_n|]$$\n",
    "\n",
    "Następnie odpowiedniemu miejscu w wektorze przypisujemy jego strone i wybieramy OutputSize największych\n",
    "\n",
    "\n",
    "*dodatkowo zostało zakomentowane rozwiązanie podpunktu 6. które jest zdecydowanie wolniejsze i odpowiada temu co się wykonuje w zadaniu 7, oraz w zad 8 nowa miara podobieństwa została zmieniona w odpowiednio ten sam sposób*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.sparse import linalg\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lab6_websiteHandler import *\n",
    "\n",
    "\n",
    "def BOWtoVector(BOW) -> sparse.csr_matrix:\n",
    "    globalBow = load_dict_from_file(\"Databases/globalBow.json\")\n",
    "    IDFs = load_dict_from_file(\"Databases/IDFs.json\")\n",
    "    SparseVector = sparse.lil_matrix((len(globalBow), 1), dtype=np.float32)\n",
    "    for word, value in BOW.items():\n",
    "        if word not in globalBow:\n",
    "            continue\n",
    "        SparseVector[globalBow[word], 0] = value*IDFs[word]\n",
    "    return sparse.csr_matrix(SparseVector.tocsr())\n",
    "\n",
    "\n",
    "def CreateTermByDocumentMatrix() -> sparse.csr_matrix:\n",
    "    print(\"===============CREATING TBD MATRIX================\")\n",
    "    URLs = load_dict_from_file(\"Databases/URLs.json\")\n",
    "    globalBow = load_dict_from_file(\"Databases/globalBow.json\")\n",
    "    URLIndexes = load_dict_from_file(\"Databases/URLIndexes.json\")\n",
    "    IDFs = load_dict_from_file(\"Databases/IDFs.json\")\n",
    "    TermByDocumentMatrix = sparse.lil_matrix(\n",
    "        (len(globalBow), len(URLs)), dtype=np.float32)\n",
    "    for url, urlDict in URLs.items():\n",
    "        for word, value in urlDict.items():\n",
    "            TermByDocumentMatrix[globalBow[word],\n",
    "                                 URLIndexes[url]] = value*IDFs[word]\n",
    "\n",
    "    TermByDocumentMatrix_CSC = TermByDocumentMatrix.tocsc()\n",
    "    column_norms = linalg.norm(TermByDocumentMatrix_CSC, axis=0)\n",
    "    normalisedTermByDocumentMatrix = TermByDocumentMatrix_CSC/column_norms\n",
    "    print(\"=======================DONE=======================\")\n",
    "    return normalisedTermByDocumentMatrix\n",
    "\n",
    "\n",
    "def initialize_svd(search_matrix, k):\n",
    "\n",
    "    print(\"SVD calculation started.\")\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    svd.fit(search_matrix)\n",
    "\n",
    "    us_matrix = svd.transform(search_matrix)\n",
    "    v_t_matrix = np.array(svd.components_)\n",
    "    print(\"SVD calculation finished.\")\n",
    "\n",
    "    with open(\"Databases/us.npz\", 'wb') as file:\n",
    "        np.save(file, us_matrix)\n",
    "    with open(\"Databases/v_t.npz\", 'wb') as file:\n",
    "        np.save(file, v_t_matrix)\n",
    "    print(\"SVD matrices saved succesfully.\")\n",
    "\n",
    "\n",
    "def calculateCosine(Query: sparse.csr_matrix, TBDMatrix: sparse.csr_matrix, Ej: sparse.csr_matrix):\n",
    "    return (Query.transpose() @ (TBDMatrix @ Ej.transpose()))/(linalg.norm(Query) * linalg.norm(TBDMatrix @ Ej.transpose()))\n",
    "\n",
    "\n",
    "def PrzeglądarkaMain(TermByDocumentMatrix, InputString: str, URLsLen, SVD=False, OutputSize=10) -> list:\n",
    "    # this slow and bad\n",
    "    # def handleQuery(Query: sparse.csr_matrix):\n",
    "    #     nonlocal TermByDocumentMatrix, URLs, IndexedURLs\n",
    "    #     QueryResult = {}\n",
    "    #     for j in range(len(URLs)):\n",
    "    #         QueryResult[IndexedURLs[str(j)]] = calculateCosine(\n",
    "    #             Query, TermByDocumentMatrix, TermByDocumentMatrix[j:j+1, :])\n",
    "    #     return QueryResult\n",
    "\n",
    "    def handleQuery2(Query: sparse.csr_matrix):\n",
    "        nonlocal TermByDocumentMatrix, IndexedURLs\n",
    "        normalisedQuery = Query/linalg.norm(Query)\n",
    "        Cosines = normalisedQuery.transpose() @ TermByDocumentMatrix\n",
    "        QueryResult = {}\n",
    "        for j in range(len(Cosines)):\n",
    "            QueryResult[IndexedURLs[str(j)]] = Cosines[0, j]\n",
    "        return QueryResult\n",
    "\n",
    "    def handleQuerySVD(Query: sparse.csr_matrix):\n",
    "        nonlocal TermByDocumentMatrix, IndexedURLs\n",
    "        us_matrix = np.load(\"Databases/us.npz\")\n",
    "        v_t_matrix = np.load(\"Databases/v_t.npz\")\n",
    "        Query /= linalg.norm(Query)\n",
    "        transposedQuery = np.transpose(Query.toarray())\n",
    "        print(transposedQuery.shape)\n",
    "        print(us_matrix.shape)\n",
    "        print(v_t_matrix.shape)\n",
    "        Cosines = (transposedQuery @ us_matrix) @ v_t_matrix\n",
    "        QueryResult = {}\n",
    "        for j in range(Cosines):\n",
    "            QueryResult[IndexedURLs[str(j)]] = Cosines[0, j]\n",
    "        return QueryResult\n",
    "\n",
    "    IndexedURLs = load_dict_from_file(\"Databases/IndexedURLs.json\")\n",
    "    if InputString == \"\":\n",
    "        return []\n",
    "\n",
    "    Query = BOWtoVector(TextToBOW(InputString))\n",
    "\n",
    "    if SVD == True:\n",
    "        QueryResult = handleQuerySVD(Query)\n",
    "    else:\n",
    "        QueryResult = handleQuery2(Query)\n",
    "    sortedQueryResult = sorted(\n",
    "        QueryResult.items(), key=lambda x: x[1], reverse=True)\n",
    "    result_list = sortedQueryResult[:OutputSize]\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krok 3. Frontend\n",
    "\n",
    "bardzo prosty frontend napisany przy użyciu Flaska zawierający 5 plików:\n",
    "1. templates/index.html\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "    <head>\n",
    "        <link\n",
    "            rel=\"stylesheet\"\n",
    "            href=\"{{ url_for('static', filename='styleIndex.css') }}\"\n",
    "        />\n",
    "        <meta charset=\"UTF-8\" />\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "        <title>Wyszukiwarka studenta</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"mainDiv\">\n",
    "            <h1><a id=\"noAnchor\" href=\"/\">Stuudentle</a></h1>\n",
    "            <form action=\"/search\" method=\"get\">\n",
    "                <div class=\"inputDiv\">\n",
    "                    <input type=\"text\" name=\"q\" />\n",
    "                    <button type=\"submit\">Submit</button>\n",
    "                    <p>Specify how much documents you want to see:</p>\n",
    "                    <input type=\"text\" name=\"outputSize\" style=\"width: 10%\" />\n",
    "                </div>\n",
    "            </form>\n",
    "            <form action=\"/remakeDatabase\" method=\"get\">\n",
    "                <button type=\"submit\">Remake Database</button>\n",
    "            </form>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "2. search.html\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "    <head>\n",
    "        <link\n",
    "            rel=\"stylesheet\"\n",
    "            href=\"{{ url_for('static', filename='styleSearch.css') }}\"\n",
    "        />\n",
    "        <meta charset=\"UTF-8\" />\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "        <title>Wyszukiwarka studenta</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"mainDiv\">\n",
    "            <h1><a id=\"noAnchor\" href=\"/\">Stuudentle</a></h1>\n",
    "            <form action=\"/search\" method=\"get\">\n",
    "                <div class=\"inputDiv\">\n",
    "                    <input type=\"text\" name=\"q\" value=\"{{ input_string }}\" />\n",
    "                    <button type=\"submit\">Submit</button>\n",
    "                    <p>Specify how much documents you want to see:</p>\n",
    "                    <input type=\"text\" name=\"outputSize\" style=\"width: 10%\" />\n",
    "                </div>\n",
    "            </form>\n",
    "            <br /><br />\n",
    "            {% if links %}\n",
    "            <h2>Search results</h2>\n",
    "            <h3>Response time: {{ response_time }}</h3>\n",
    "            <br />\n",
    "            <ul>\n",
    "                {% for link in links %}\n",
    "                <li>\n",
    "                    <p>[{{ link.value }}]</p>\n",
    "                    <a href=\"{{ link.url }}\">{{ link.url }}</a>\n",
    "                </li>\n",
    "                {% endfor %}\n",
    "            </ul>\n",
    "            {% endif %}\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "Odpowiadające im pliki css:\n",
    "\n",
    "3. styleIndex.css\n",
    "```css\n",
    "body {\n",
    "    background-color: #202124; \n",
    "    font-family: Arial, sans-serif;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "}\n",
    "\n",
    ".container {\n",
    "    max-width: 800px;\n",
    "    margin: 0 auto;\n",
    "    padding: 20px;\n",
    "}\n",
    "\n",
    "h1 {\n",
    "    color: #ffffff; \n",
    "    font-size: 400%;\n",
    "}\n",
    "\n",
    "form {\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    "input[type=\"text\"] {\n",
    "    padding: 10px;\n",
    "    width: 70%;\n",
    "    background-color: #292929; \n",
    "    color: #ffffff; \n",
    "    border: 1px solid #555; \n",
    "}\n",
    "/*...*/\n",
    "```\n",
    "4. styleSearch.css\n",
    "```css\n",
    "body {\n",
    "    background-color: #202124;\n",
    "    font-family: Arial, sans-serif;\n",
    "    margin: 0;\n",
    "    padding: 0;\n",
    "}\n",
    "\n",
    ".container {\n",
    "    max-width: 800px;\n",
    "    margin: 0 auto;\n",
    "    padding: 20px;\n",
    "}\n",
    "\n",
    "h1 {\n",
    "    color: #ffffff;\n",
    "    font-size: 250%;\n",
    "}\n",
    "\n",
    "form {\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    "input[type=\"text\"] {\n",
    "    padding: 10px;\n",
    "    width: 70%;\n",
    "    background-color: #292929;\n",
    "    color: #ffffff;\n",
    "    border: 1px solid #555;\n",
    "}\n",
    "\n",
    "button {\n",
    "    padding: 10px 20px;\n",
    "    background-color: #1a73e8;\n",
    "    color: #ffffff;\n",
    "    border: none;\n",
    "    cursor: pointer;\n",
    "}\n",
    "/*...*/\n",
    "\n",
    "```\n",
    "\n",
    "Oraz flaskapp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, redirect, url_for\n",
    "from lab6 import CreateTermByDocumentMatrix, PrzeglądarkaMain, initialize_svd\n",
    "from lab6_websiteHandler import createBOWs, calculateIDFs, IndexUrls, load_dict_from_file\n",
    "import time\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# . .venv/bin/activate !!!!!!!!!!!!!!!!!!!!!!\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "# Create the Term-By-Document Matrix (TBDM) and store it in the app configuration\n",
    "app.config['TBDM'] = CreateTermByDocumentMatrix()\n",
    "\n",
    "\n",
    "def process_links(resultList):\n",
    "    links = []\n",
    "    if resultList is None:\n",
    "        return\n",
    "    for link, sparseMatrixValue in resultList:\n",
    "        link = {'url': link,\n",
    "                'value': str(round(sparseMatrixValue*100, 2)) + '%'}\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "\n",
    "def is_integer_string(s):\n",
    "    return s.isdigit()\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    input_string = \"\"\n",
    "    if request.method == 'POST':\n",
    "        input_string = request.form['input_string']\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/search', methods=['GET', 'POST'])\n",
    "def search():\n",
    "    # Retrieve the TBDM from the app configuration or initialize it if not present\n",
    "    TBDM = app.config.get('TBDM')\n",
    "    result = None\n",
    "    time_taken = None\n",
    "    k = 10\n",
    "    if request.method == 'GET':\n",
    "        URLs = load_dict_from_file(\"Databases/URLs.json\")\n",
    "        query = request.args.get('q', '')\n",
    "        k = request.args.get('outputSize', 10)\n",
    "        if not is_integer_string(k):\n",
    "            k = 10\n",
    "        start = time.time()\n",
    "        result = PrzeglądarkaMain(TBDM, query, len(URLs), OutputSize=int(k))\n",
    "        end = time.time()\n",
    "        time_taken = str(round(end-start, 4)) + \"s\"\n",
    "    return render_template('search.html', links=process_links(result), input_string=query, response_time=time_taken)\n",
    "\n",
    "\n",
    "@app.route('/remakeDatabase')\n",
    "def remakeDatabase():\n",
    "    TBDM = app.config.get('TBDM')\n",
    "    createBOWs()\n",
    "    calculateIDFs()\n",
    "    IndexUrls()\n",
    "    initialize_svd(TBDM, 100)\n",
    "    return redirect(url_for('index'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki:\n",
    "\n",
    "Czas trwania: \n",
    "- Części przygotowania danych poniżej 20 minut,\n",
    "- Części uruchomienia przeglądarki (tworzenie macierzy) około 10 sekund\n",
    "- Części liczenia rankingu dokumentów niecała sekunda\n",
    "\n",
    "Wyniki:\n",
    "- Z SVD (najlepsze wyniki koło k = 100): najlepsze wyszukania - wyniki w granicach 20-30%\n",
    "- Bez SVD: wyszukania średnio 10-15% lepsze\n",
    "\n",
    "IDF: nieznacząco wydłuża czas wyszukiwania, ale zwiększa dokładośc o około 5% względem wersji bez"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
